{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Alankrita_NLP_RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNwbY7O8AsV3",
        "colab_type": "text"
      },
      "source": [
        "# Implementation of Spam Classifier by simple RNN model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ8Uq-QHAryZ",
        "colab_type": "text"
      },
      "source": [
        "In order to get a better understanding of RNN we'll try to understand it with an example , here we are going to build a simple RNN model that classifies message into SPAM or HAM\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "I have implemented my code on google colab notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpnVpGf6r3Y1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing drive \n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NdvBqU6r7dV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fbe28484-8092-43de-fb57-a473c02c1b8e"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puOQPG5FsnoU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing pandas for reading csv file from drive\n",
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqIJ8WjVsSzD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_data= pd.read_csv(\"/content/drive/My Drive/SMSCollection.txt\",sep='\\t',names=['label','text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0DihYEYsyaw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "7fe098fa-bbc7-42ed-aef6-a705b34cf27d"
      },
      "source": [
        "\n",
        "training_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                               text\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3YrwoSOB0_4",
        "colab_type": "text"
      },
      "source": [
        "This is how are data looks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVsSamxiDhR_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e80b782f-936f-4c07-9437-b28716650083"
      },
      "source": [
        "#checking the shape of dataset\n",
        "training_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5572, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO0rkkFRvmmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeb29Z1xCEhQ",
        "colab_type": "text"
      },
      "source": [
        "This is a function to clean text withe the help of re or Regular expression library (*A regular expression is a special sequence of characters that helps you match or find other strings or sets of strings, using a specialized syntax held in a pattern*) re,sub removes the special characters and urls from the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snw5W7j4tukb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
        "    and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyPe2Tl8u7cl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_data['text'] = training_data['text'].apply(lambda x: clean_text(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIWZTmysC5hA",
        "colab_type": "text"
      },
      "source": [
        "Label Encoding the labels in the dataset **Ham:0, Spam:1**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAWS_MPnypCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "lb= LabelEncoder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyZuzRDDyw4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_data['label'] = lb.fit_transform(training_data['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKDnOrtGDMNb",
        "colab_type": "text"
      },
      "source": [
        "Our modified dataset looks like this.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biLFS8onxnwA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "57b3f9d0-60e1-45fc-abb7-faac9eeeb001"
      },
      "source": [
        "training_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>go until jurong point crazy available only in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>ok lar joking wif u oni</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>free entry in  a wkly comp to win fa cup final...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>u dun say so early hor u c already then say</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                               text\n",
              "0      0  go until jurong point crazy available only in ...\n",
              "1      0                            ok lar joking wif u oni\n",
              "2      1  free entry in  a wkly comp to win fa cup final...\n",
              "3      0        u dun say so early hor u c already then say\n",
              "4      0  nah i dont think he goes to usf he lives aroun..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRHA8U3rDV0G",
        "colab_type": "text"
      },
      "source": [
        "Now we'll convert our modified dataset into CSV file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3km3F17AvtMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "training_data.to_csv('train.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZkymyxpDhOA",
        "colab_type": "text"
      },
      "source": [
        "# Getting started with Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HE6s5EyDDo70",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Importing Necessary Pytorch libraries for dealing with text data**\n",
        "\n",
        "* Torch package is used to define tensors and mathematical operations on it\n",
        "* TorchText is a Natural Language Processing (NLP) library in PyTorch. This     library contains the scripts for preprocessing text and source of few popular   NLP datasets.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XP-nLKtlB_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext import data \n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UqzjzpxEZ_X",
        "colab_type": "text"
      },
      "source": [
        "Setting device to GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4JgJkp1l2Mh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMCRRiXgFlzE",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing data in Pytorch\n",
        "\n",
        "---\n",
        "\n",
        "torchtext.data provides a **Field** object which use to preprocess data,Every dataset consists of one or more types of data. For instance, a text classification dataset contains sentences and their classes(labels)\n",
        "*`Field class models common text processing datatypes that can be represented by tensors. It holds a Vocab object that defines the set of possible values for elements of the field and their corresponding numerical representations. The Field object also holds other parameters relating to how a datatype should be numericalized, such as a tokenization method and the kind of Tensor that should be produced`*\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Here we'll define \n",
        "* TEXT field with parametrs as tokenizer,(spacy since it uses novel tokenization algorithm), batch_first keeps the first dimension of input and output as batch size\n",
        "* LabelField : converts the label into torch tensor float.\n",
        "\n",
        "\n",
        "**Thus our dataset will have 2 columns as label and text**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ9LxrNxuoNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT = data.Field(tokenize='spacy',batch_first=True,include_lengths=True)\n",
        "LABEL = data.LabelField(dtype = torch.float,batch_first=True)\n",
        "fields = [('label', LABEL), ('text',TEXT)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQVDWLmzH_DV",
        "colab_type": "text"
      },
      "source": [
        "TabularDataset is used to load a custom data set it takes path of csv file as a input, fields are the list of tuples defining text and label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRrSzQcfuonW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "556c7b82-1409-4d46-c12a-b820885d4576"
      },
      "source": [
        "#loading custom dataset\n",
        "training_data=data.TabularDataset(path = 'train.csv',format = 'csv',fields = fields,skip_header = True)\n",
        "\n",
        "#print preprocessed text\n",
        "print(vars(training_data.examples[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'label': '0', 'text': ['go', 'until', 'jurong', 'point', 'crazy', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'there', 'got', 'amore', 'wat']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCnCW2YIwtE6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ec036cff-64eb-496a-810a-7650d4aaae8f"
      },
      "source": [
        "#seeding our program\n",
        "SEED= 42\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f37fa0f11b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbArbc8II1cv",
        "colab_type": "text"
      },
      "source": [
        "splitting the dataset into training and validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ileSH6T9uohG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "train_data, valid_data = training_data.split(split_ratio=0.7, random_state = random.seed(SEED))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIf1P8BMJYxP",
        "colab_type": "text"
      },
      "source": [
        "The next step is to build the vocabulary for the text and convert them into integer sequences. **Vocabulary contains the unique words in the entire text. Each unique word is assigned an index**.  build_vocab function construct the Vocab object for this field from the dataset.\n",
        "We are building vocabulary with pre trained glove embeddings\n",
        "Two special tokens known as unknown and padding will be added to the vocabulary\n",
        "* Unknown token is used to handle Out Of Vocabulary words\n",
        "* Padding token is used to make input sequences of same length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3u5ltLqo7K5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "ce0ca4a8-a79c-41c8-8e7f-30222d36e144"
      },
      "source": [
        "#initialize glove embeddings\n",
        "TEXT.build_vocab(train_data,min_freq=3,vectors = \"glove.6B.100d\")  \n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "#No. of unique tokens in text\n",
        "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
        "\n",
        "#No. of unique tokens in label\n",
        "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n",
        "\n",
        "#Commonly used words\n",
        "print(TEXT.vocab.freqs.most_common(10))  \n",
        "\n",
        "#Word dictionary\n",
        "print(TEXT.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:26, 2.23MB/s]                           \n",
            " 99%|█████████▉| 397703/400000 [00:17<00:00, 23091.78it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Size of TEXT vocabulary: 2070\n",
            "Size of LABEL vocabulary: 2\n",
            "[(' ', 2704), ('i', 1995), ('to', 1609), ('you', 1556), ('a', 1032), ('the', 929), ('u', 766), ('and', 671), ('is', 648), ('in', 606)]\n",
            "defaultdict(<function _default_unk_index at 0x7f37af8b4840>, {'<unk>': 0, '<pad>': 1, ' ': 2, 'i': 3, 'to': 4, 'you': 5, 'a': 6, 'the': 7, 'u': 8, 'and': 9, 'is': 10, 'in': 11, 'me': 12, 'for': 13, 'my': 14, 'your': 15, 'do': 16, 'nt': 17, 'have': 18, 'it': 19, 'of': 20, 'that': 21, 'call': 22, 'on': 23, 'are': 24, 'm': 25, 'now': 26, 'not': 27, 'so': 28, 'or': 29, 'but': 30, 'can': 31, 'at': 32, 'ur': 33, 'will': 34, 'get': 35, 'be': 36, 'just': 37, 'with': 38, 'if': 39, 'we': 40, 'no': 41, 'this': 42, '£': 43, 'its': 44, 's': 45, 'how': 46, 'free': 47, 'when': 48, 'go': 49, 'what': 50, 'ok': 51, 'up': 52, 'all': 53, '  ': 54, 'from': 55, 'know': 56, 'out': 57, 'got': 58, 'ltgt': 59, 'ill': 60, 'like': 61, 'there': 62, 'come': 63, 'was': 64, 'am': 65, 'good': 66, 'then': 67, 'love': 68, 'he': 69, 'day': 70, 'text': 71, 'send': 72, 'time': 73, 'only': 74, 'want': 75, 'by': 76, 'did': 77, 'txt': 78, 'going': 79, 'as': 80, 'about': 81, 'need': 82, 'one': 83, 'd': 84, 'our': 85, 'home': 86, 'back': 87, 'lor': 88, 'still': 89, 'ü': 90, 'stop': 91, 'sorry': 92, 'mobile': 93, 'hi': 94, 'reply': 95, 'tell': 96, 'see': 97, 'n': 98, 'r': 99, 'who': 100, 'she': 101, 'today': 102, 'later': 103, 'well': 104, 'much': 105, 'phone': 106, 'any': 107, 'been': 108, 'da': 109, 'please': 110, 'an': 111, 'they': 112, 'think': 113, 'some': 114, 'new': 115, 'week': 116, 'ca': 117, 'night': 118, 'take': 119, 'great': 120, 'has': 121, 'her': 122, 'here': 123, 'hey': 124, 'hope': 125, 'claim': 126, 'happy': 127, 'make': 128, 'more': 129, 'oh': 130, '   ': 131, 'him': 132, 'way': 133, 'should': 134, 'work': 135, 'where': 136, 'dear': 137, 'give': 138, 'too': 139, 've': 140, 'had': 141, 'pls': 142, 'already': 143, 'tomorrow': 144, 'message': 145, 'wat': 146, 'yeah': 147, 'after': 148, 'prize': 149, 're': 150, 'cash': 151, 'doing': 152, 'msg': 153, 'right': 154, 'say': 155, 'yes': 156, 'meet': 157, 'number': 158, 'said': 159, 'amp': 160, 'ask': 161, 'really': 162, 'every': 163, 'lol': 164, 'why': 165, 'e': 166, 'them': 167, 'also': 168, 'something': 169, 'cos': 170, 'c': 171, 'find': 172, 'service': 173, 'contact': 174, 'k': 175, 'life': 176, 'miss': 177, 'were': 178, 'win': 179, 'won': 180, 'babe': 181, 'last': 182, 'nokia': 183, 'sent': 184, 'sure': 185, 'thanks': 186, 'chat': 187, 'morning': 188, 'tonight': 189, 'which': 190, 'always': 191, 'around': 192, 'even': 193, 'keep': 194, 'na': 195, 'over': 196, 'someone': 197, 'could': 198, 'feel': 199, 'would': 200, 'anything': 201, 'first': 202, 'off': 203, 'pick': 204, 'very': 205, 'wait': 206, 'again': 207, 'b': 208, 'customer': 209, 'his': 210, 'sleep': 211, 'soon': 212, 'many': 213, 'urgent': 214, 'late': 215, 'next': 216, 'went': 217, 'buy': 218, 'care': 219, 'down': 220, 'help': 221, 'place': 222, 'ya': 223, 'money': 224, 'us': 225, 'does': 226, 'dun': 227, 'friends': 228, 'gon': 229, 'leave': 230, 'let': 231, 'x': 232, 'thing': 233, 'wo': 234, 'other': 235, 'told': 236, 'waiting': 237, 'haha': 238, 'may': 239, 'try': 240, 'guaranteed': 241, 'gud': 242, 'never': 243, 'things': 244, 'tone': 245, 'better': 246, 'coming': 247, 'lunch': 248, 'nice': 249, 'people': 250, 'same': 251, 'use': 252, 'wish': 253, 'year': 254, 'getting': 255, 'live': 256, 'mins': 257, 'fine': 258, 'hello': 259, 'latest': 260, 'per': 261, 'wan': 262, 'wanna': 263, 'before': 264, 'cool': 265, 'days': 266, 'draw': 267, 'smile': 268, 'special': 269, 'bit': 270, 'done': 271, 'holiday': 272, 'long': 273, 'meeting': 274, 'person': 275, 'thought': 276, 'yup': 277, 'being': 278, 'best': 279, 'class': 280, 'having': 281, 'heart': 282, 'man': 283, 'name': 284, 'real': 285, 'talk': 286, 'than': 287, 'thk': 288, 'trying': 289, 'v': 290, 'yet': 291, 'yo': 292, 'check': 293, 'enjoy': 294, 'few': 295, 'job': 296, 'landline': 297, 'camera': 298, 'chance': 299, 'guess': 300, 'problem': 301, 'word': 302, 'friend': 303, 'house': 304, 'luv': 305, 'nothing': 306, 'ready': 307, 'receive': 308, 'shit': 309, 'speak': 310, 'account': 311, 'because': 312, 'cost': 313, 'dinner': 314, 'face': 315, 'finish': 316, 'girl': 317, 'god': 318, 'half': 319, 'liao': 320, 'min': 321, 'mind': 322, 'play': 323, 'stuff': 324, 'y': 325, 'awarded': 326, 'boy': 327, 'called': 328, 'dis': 329, 'guys': 330, 'into': 331, 'offer': 332, 'orange': 333, 'sms': 334, 'watch': 335, 'weekend': 336, 'another': 337, 'box': 338, 'dat': 339, 'end': 340, 'fuck': 341, 'jus': 342, 'line': 343, 'maybe': 344, 'month': 345, 'part': 346, 'po': 347, 'shows': 348, 'start': 349, 'actually': 350, 'aight': 351, 'car': 352, 'dunno': 353, 'eat': 354, 'hear': 355, 'leh': 356, 'little': 357, 'lot': 358, 'probably': 359, 'quite': 360, 'sir': 361, 'sweet': 362, 'thank': 363, 'town': 364, 'ah': 365, 'anyway': 366, 'apply': 367, 'asked': 368, 'baby': 369, 'bad': 370, 'big': 371, 'bus': 372, 'ever': 373, 'everything': 374, 'pa': 375, 'pay': 376, 'reach': 377, 'ringtone': 378, 'school': 379, 'shall': 380, 'video': 381, 'wif': 382, 'between': 383, 'came': 384, 'code': 385, 'den': 386, 'entry': 387, 'fun': 388, 'looking': 389, 'mail': 390, 'might': 391, 'remember': 392, 'room': 393, 'stay': 394, 'thanx': 395, 'until': 396, 'world': 397, 'birthday': 398, 'bring': 399, 'forgot': 400, 'hour': 401, 'lar': 402, 'missing': 403, 'okay': 404, 'plan': 405, 'princess': 406, 'texts': 407, 'tv': 408, 'update': 409, 'wanted': 410, 'while': 411, 'without': 412, 'abt': 413, 'bed': 414, 'change': 415, 'easy': 416, 'hav': 417, 'kiss': 418, 'left': 419, 'look': 420, 'making': 421, 'means': 422, 'minutes': 423, 'most': 424, 'network': 425, 'office': 426, 'once': 427, 'plus': 428, 'put': 429, 'selected': 430, 'sexy': 431, 'shop': 432, 'shopping': 433, 'since': 434, 'tcs': 435, 'together': 436, 'tones': 437, 'true': 438, 'yesterday': 439, 'afternoon': 440, 'attempt': 441, 'award': 442, 'bt': 443, 'colour': 444, 'dad': 445, 'details': 446, 'driving': 447, 'dude': 448, 'either': 449, 'else': 450, 'g': 451, 'game': 452, 'gift': 453, 'important': 454, 'join': 455, 'll': 456, 'mob': 457, 'plz': 458, 'sat': 459, 'sch': 460, 'though': 461, 'tmr': 462, 'trip': 463, 'two': 464, 'watching': 465, 'wk': 466, 'ard': 467, 'busy': 468, 'collect': 469, 'comes': 470, 'double': 471, 'early': 472, 'email': 473, 'family': 474, 'goes': 475, 'guy': 476, 'juz': 477, 'lei': 478, 'lose': 479, 'made': 480, 'pain': 481, 'says': 482, 'till': 483, 'todays': 484, 'weekly': 485, 'working': 486, 'worry': 487, 'xmas': 488, 'xxx': 489, 'years': 490, 'yourself': 491, 'alright': 492, 'angry': 493, 'awesome': 494, 'beautiful': 495, 'bored': 496, 'collection': 497, 'de': 498, 'enough': 499, 'evening': 500, 'hair': 501, 'ltdecimalgt': 502, 'mean': 503, 'missed': 504, 'must': 505, 'o': 506, 'oso': 507, 'points': 508, 'show': 509, 'tc': 510, 'those': 511, 'top': 512, 'wen': 513, 'wot': 514, 'address': 515, 'bonus': 516, 'both': 517, 'calls': 518, 'coz': 519, 'dating': 520, 'delivery': 521, 'dreams': 522, 'drive': 523, 'final': 524, 'friendship': 525, 'hurt': 526, 'music': 527, 'needs': 528, 'news': 529, 'pobox': 530, 'pounds': 531, 'price': 532, 'ring': 533, 'sad': 534, 'started': 535, 'thinking': 536, 'til': 537, 'tot': 538, 'wants': 539, 'words': 540, 'aft': 541, 'book': 542, 'choose': 543, 'drop': 544, 'each': 545, 'feeling': 546, 'girls': 547, 'haf': 548, 'happen': 549, 'kind': 550, 'lets': 551, 'lots': 552, 'old': 553, 'poly': 554, 'private': 555, 'run': 556, 'saying': 557, 'sis': 558, 'story': 559, 'takes': 560, 'these': 561, 'times': 562, 'took': 563, 'touch': 564, 'treat': 565, 'tried': 566, 'wake': 567, 'weeks': 568, 'wid': 569, '    ': 570, 'able': 571, 'alone': 572, 'auction': 573, 'away': 574, 'boytoy': 575, 'break': 576, 'brother': 577, 'close': 578, 'club': 579, 'date': 580, 'hot': 581, 'leaving': 582, 'mum': 583, 'neva': 584, 'nite': 585, 'noe': 586, 'rite': 587, 'sae': 588, 'search': 589, 'set': 590, 'side': 591, 'sister': 592, 'sleeping': 593, 'statement': 594, 'unsubscribe': 595, 'voucher': 596, 'vouchers': 597, 'walk': 598, 'whatever': 599, 'wit': 600, 'yours': 601, 'answer': 602, 'anytime': 603, 'believe': 604, 'carlos': 605, 'credit': 606, 'cut': 607, 'decided': 608, 'everyone': 609, 'expires': 610, 'fancy': 611, 'finished': 612, 'food': 613, 'found': 614, 'fri': 615, 'full': 616, 'hard': 617, 'hours': 618, 'identifier': 619, 'knw': 620, 'lesson': 621, 'light': 622, 'makes': 623, 'messages': 624, 'operator': 625, 'post': 626, 'rate': 627, 'read': 628, 'saw': 629, 'secret': 630, 'services': 631, 'smoke': 632, 'sun': 633, 'ta': 634, 'tscs': 635, 'unredeemed': 636, 'valid': 637, 'w': 638, 'ago': 639, 'anyone': 640, 'area': 641, 'ass': 642, 'await': 643, 'bout': 644, 'card': 645, 'cause': 646, 'congrats': 647, 'content': 648, 'course': 649, 'darlin': 650, 'drink': 651, 'eve': 652, 'fast': 653, 'friday': 654, 'frnd': 655, 'frnds': 656, 'fucking': 657, 'gd': 658, 'goin': 659, 'gym': 660, 'ha': 661, 'hell': 662, 'hit': 663, 'hungry': 664, 'invited': 665, 'lovely': 666, 'lucky': 667, 'mate': 668, 'motorola': 669, 'movie': 670, 'online': 671, 'order': 672, 'party': 673, 'phones': 674, 'pic': 675, 'pics': 676, 'pm': 677, 'pub': 678, 'safe': 679, 'smth': 680, 'sounds': 681, 'swing': 682, 'tel': 683, 'test': 684, 'thinks': 685, 'tho': 686, 'tomo': 687, 'truth': 688, 'wats': 689, 'whenever': 690, 'available': 691, 'bcoz': 692, 'bill': 693, 'booked': 694, 'calling': 695, 'charge': 696, 'couple': 697, 'cum': 698, 'energy': 699, 'extra': 700, 'far': 701, 'finally': 702, 'forget': 703, 'games': 704, 'goodmorning': 705, 'hand': 706, 'happened': 707, 'happiness': 708, 'head': 709, 'hee': 710, 'hmm': 711, 'hold': 712, 'huh': 713, 'info': 714, 'information': 715, 'knew': 716, 'land': 717, 'listen': 718, 'log': 719, 'lost': 720, 'loved': 721, 'loving': 722, 'march': 723, 'minute': 724, 'mobiles': 725, 'months': 726, 'mu': 727, 'muz': 728, 'nope': 729, 'offers': 730, 'opt': 731, 'oredi': 732, 'paper': 733, 'prob': 734, 'project': 735, 'representative': 736, 'saturday': 737, 'second': 738, 'seeing': 739, 'semester': 740, 'smiling': 741, 'sort': 742, 't': 743, 'taking': 744, 'telling': 745, 'used': 746, 'visit': 747, 'welcome': 748, 'wife': 749, 'winner': 750, 'wkly': 751, 'worth': 752, 'yr': 753, 'access': 754, 'almost': 755, 'ans': 756, 'asking': 757, 'bank': 758, 'bday': 759, 'bslvyl': 760, 'camcorder': 761, 'complimentary': 762, 'cs': 763, 'direct': 764, 'dream': 765, 'earlier': 766, 'eg': 767, 'eh': 768, 'entered': 769, 'ex': 770, 'feels': 771, 'felt': 772, 'flag': 773, 'fr': 774, 'gets': 775, 'gettin': 776, 'gone': 777, 'hoping': 778, 'kate': 779, 'king': 780, 'least': 781, 'loan': 782, 'lovable': 783, 'luck': 784, 'mah': 785, 'meant': 786, 'moment': 787, 'msgs': 788, 'myself': 789, 'nah': 790, 'near': 791, 'okie': 792, 'ones': 793, 'optout': 794, 'outside': 795, 'pete': 796, 'player': 797, 'pretty': 798, 'question': 799, 'reading': 800, 'reason': 801, 'sea': 802, 'sex': 803, 'snow': 804, 'somebody': 805, 'st': 806, 'talking': 807, 'through': 808, 'txting': 809, 'txts': 810, 'type': 811, 'ugh': 812, 'uncle': 813, 'understand': 814, 'unlimited': 815, 'ure': 816, 'valued': 817, 'via': 818, 'whole': 819, 'wil': 820, 'wine': 821, 'wonder': 822, 'abiola': 823, 'admirer': 824, 'against': 825, 'bluetooth': 826, 'brings': 827, 'buying': 828, 'caller': 829, 'callertune': 830, 'case': 831, 'charged': 832, 'checking': 833, 'christmas': 834, 'college': 835, 'comp': 836, 'company': 837, 'confirm': 838, 'congratulations': 839, 'copy': 840, 'correct': 841, 'crave': 842, 'currently': 843, 'deal': 844, 'deep': 845, 'die': 846, 'dogging': 847, 'doin': 848, 'england': 849, 'enter': 850, 'etc': 851, 'exam': 852, 'eyes': 853, 'fantastic': 854, 'freephone': 855, 'frm': 856, 'gal': 857, 'gas': 858, 'gods': 859, 'heard': 860, 'ho': 861, 'hospital': 862, 'immediately': 863, 'kinda': 864, 'laptop': 865, 'laugh': 866, 'ldn': 867, 'leaves': 868, 'lect': 869, 'loves': 870, 'ltd': 871, 'match': 872, 'mates': 873, 'mayb': 874, 'mm': 875, 'mr': 876, 'nyt': 877, 'otherwise': 878, 'parents': 879, 'park': 880, 'plans': 881, 'power': 882, 'press': 883, 'promise': 884, 'quiz': 885, 'remove': 886, 'replying': 887, 'rs': 888, 'savamob': 889, 'save': 890, 'sending': 891, 'seriously': 892, 'short': 893, 'slow': 894, 'song': 895, 'sound': 896, 'spend': 897, 'staying': 898, 'studying': 899, 'support': 900, 'surprise': 901, 'ten': 902, 'terms': 903, 'tired': 904, 'uk': 905, 'uks': 906, 'valentine': 907, 'wednesday': 908, 'weekends': 909, 'within': 910, 'wonderful': 911, 'worried': 912, 'wow': 913, 'yar': 914, 'yep': 915, '…': 916, 'accept': 917, 'add': 918, 'age': 919, 'ai': 920, 'ave': 921, 'bak': 922, 'bathe': 923, 'bid': 924, 'blue': 925, 'boss': 926, 'cd': 927, 'cheers': 928, 'chennai': 929, 'chikku': 930, 'comin': 931, 'completely': 932, 'computer': 933, 'crazy': 934, 'cup': 935, 'darren': 936, 'dead': 937, 'difficult': 938, 'din': 939, 'disturb': 940, 'dnt': 941, 'drugs': 942, 'eating': 943, 'entitled': 944, 'especially': 945, 'excellent': 946, 'fixed': 947, 'fone': 948, 'forever': 949, 'future': 950, 'glad': 951, 'goodnight': 952, 'grins': 953, 'heavy': 954, 'high': 955, 'hmv': 956, 'hop': 957, 'idea': 958, 'india': 959, 'ipod': 960, 'itself': 961, 'jay': 962, 'kids': 963, 'knows': 964, 'lazy': 965, 'lemme': 966, 'less': 967, 'liked': 968, 'loads': 969, 'longer': 970, 'meh': 971, 'men': 972, 'merry': 973, 'mine': 974, 'miracle': 975, 'mode': 976, 'move': 977, 'mrt': 978, 'national': 979, 'normal': 980, 'notice': 981, 'omg': 982, 'opinion': 983, 'own': 984, 'pass': 985, 'paying': 986, 'photo': 987, 'picked': 988, 'pizza': 989, 'point': 990, 'polys': 991, 'poor': 992, 'quality': 993, 'rain': 994, 'reached': 995, 'rent': 996, 'rest': 997, 'reward': 998, 'rock': 999, 'round': 1000, 'rply': 1001, 'rreveal': 1002, 'seems': 1003, 'seen': 1004, 'sell': 1005, 'showing': 1006, 'sim': 1007, 'simple': 1008, 'sitting': 1009, 'slowly': 1010, 'small': 1011, 'somewhere': 1012, 'sony': 1013, 'specialcall': 1014, 'specially': 1015, 'starting': 1016, 'std': 1017, 'street': 1018, 'study': 1019, 'stupid': 1020, 'their': 1021, 'thru': 1022, 'tht': 1023, 'tonite': 1024, 'train': 1025, 'ufind': 1026, 'under': 1027, 'unless': 1028, 'uve': 1029, 'wana': 1030, 'water': 1031, 'weight': 1032, 'wishing': 1033, 'wiv': 1034, 'wondering': 1035, 'workin': 1036, 'wrong': 1037, 'xx': 1038, 'yahoo': 1039, 'across': 1040, 'aiyah': 1041, 'apartment': 1042, 'ar': 1043, 'asap': 1044, 'awaiting': 1045, 'barely': 1046, 'bother': 1047, 'buzz': 1048, 'charity': 1049, 'cinema': 1050, 'coffee': 1051, 'country': 1052, 'credits': 1053, 'cute': 1054, 'daddy': 1055, 'decide': 1056, 'del': 1057, 'digital': 1058, 'don': 1059, 'door': 1060, 'download': 1061, 'dvd': 1062, 'ends': 1063, 'except': 1064, 'fact': 1065, 'fat': 1066, 'film': 1067, 'fingers': 1068, 'flirt': 1069, 'forward': 1070, 'freemsg': 1071, 'gave': 1072, 'giving': 1073, 'goto': 1074, 'gt': 1075, 'hiya': 1076, 'hmmm': 1077, 'hotel': 1078, 'hw': 1079, 'ice': 1080, 'il': 1081, 'inc': 1082, 'inside': 1083, 'interested': 1084, 'kb': 1085, 'kept': 1086, 'lessons': 1087, 'link': 1088, 'list': 1089, 'lover': 1090, 'lttimegt': 1091, 'mark': 1092, 'mo': 1093, 'mon': 1094, 'monday': 1095, 'movies': 1096, 'mrng': 1097, 'nobody': 1098, 'noon': 1099, 'ntt': 1100, 'numbers': 1101, 'official': 1102, 'open': 1103, 'orchard': 1104, 'paid': 1105, 'past': 1106, 'pc': 1107, 'picking': 1108, 'pix': 1109, 'planning': 1110, 'pleasure': 1111, 'police': 1112, 'problems': 1113, 'reaching': 1114, 'recently': 1115, 'registered': 1116, 'rental': 1117, 'road': 1118, 'rose': 1119, 'self': 1120, 'single': 1121, 'slept': 1122, 'south': 1123, 'sport': 1124, 'spree': 1125, 'stand': 1126, 'starts': 1127, 'store': 1128, 'style': 1129, 'sunday': 1130, 'sunshine': 1131, 'supposed': 1132, 'surely': 1133, 'system': 1134, 'tear': 1135, 'th': 1136, 'thinkin': 1137, 'track': 1138, 'trouble': 1139, 'trust': 1140, 'user': 1141, 'usf': 1142, 'using': 1143, 'vary': 1144, 'vl': 1145, 'voice': 1146, 'warm': 1147, 'waste': 1148, 'write': 1149, 'xy': 1150, 'yest': 1151, 'yoga': 1152, 'yrs': 1153, 'added': 1154, 'aha': 1155, 'amt': 1156, 'anymore': 1157, 'appreciate': 1158, 'arrive': 1159, 'askd': 1160, 'askin': 1161, 'ate': 1162, 'awake': 1163, 'balance': 1164, 'battery': 1165, 'behind': 1166, 'bin': 1167, 'body': 1168, 'boost': 1169, 'cashbalance': 1170, 'cashin': 1171, 'catch': 1172, 'centre': 1173, 'character': 1174, 'cheap': 1175, 'childish': 1176, 'cine': 1177, 'co': 1178, 'complete': 1179, 'contacted': 1180, 'contract': 1181, 'cover': 1182, 'custcare': 1183, 'cuz': 1184, 'dai': 1185, 'daily': 1186, 'damn': 1187, 'definitely': 1188, 'different': 1189, 'dint': 1190, 'directly': 1191, 'discount': 1192, 'dog': 1193, 'empty': 1194, 'envelope': 1195, 'exactly': 1196, 'exciting': 1197, 'excuse': 1198, 'experience': 1199, 'f': 1200, 'father': 1201, 'feb': 1202, 'fight': 1203, 'five': 1204, 'flower': 1205, 'following': 1206, 'frens': 1207, 'gay': 1208, 'ge': 1209, 'gentle': 1210, 'group': 1211, 'hai': 1212, 'happening': 1213, 'hook': 1214, 'hopefully': 1215, 'horny': 1216, 'however': 1217, 'howz': 1218, 'hr': 1219, 'hrs': 1220, 'hurry': 1221, 'hurts': 1222, 'hv': 1223, 'ignore': 1224, 'imma': 1225, 'instead': 1226, 'interesting': 1227, 'inviting': 1228, 'ish': 1229, 'j': 1230, 'john': 1231, 'joined': 1232, 'joking': 1233, 'kallis': 1234, 'lady': 1235, 'lift': 1236, 'linerental': 1237, 'lookin': 1238, 'loyalty': 1239, 'married': 1240, 'matches': 1241, 'matter': 1242, 'maximize': 1243, 'member': 1244, 'menu': 1245, 'mistake': 1246, 'moan': 1247, 'mom': 1248, 'moms': 1249, 'moon': 1250, 'moral': 1251, 'murder': 1252, 'murdered': 1253, 'murderer': 1254, 'nature': 1255, 'nigeria': 1256, 'num': 1257, 'nw': 1258, 'nxt': 1259, 'obviously': 1260, 'omw': 1261, 'onto': 1262, 'oops': 1263, 'password': 1264, 'personal': 1265, 'persons': 1266, 'photos': 1267, 'planned': 1268, 'playing': 1269, 'pound': 1270, 'pray': 1271, 'quick': 1272, 'rates': 1273, 'rather': 1274, 'rcvd': 1275, 'realy': 1276, 'receipt': 1277, 'reference': 1278, 'regards': 1279, 'relax': 1280, 'rem': 1281, 'request': 1282, 'review': 1283, 'role': 1284, 'searching': 1285, 'sense': 1286, 'share': 1287, 'shower': 1288, 'shuhui': 1289, 'sign': 1290, 'silent': 1291, 'simply': 1292, 'sing': 1293, 'singles': 1294, 'sipix': 1295, 'sky': 1296, 'slave': 1297, 'social': 1298, 'sofa': 1299, 'sometimes': 1300, 'spent': 1301, 'straight': 1302, 'sub': 1303, 'sucks': 1304, 'summer': 1305, 'tat': 1306, 'tea': 1307, 'tenerife': 1308, 'texting': 1309, 'tickets': 1310, 'totally': 1311, 'towards': 1312, 'training': 1313, 'tuesday': 1314, 'ull': 1315, 'uni': 1316, 'usual': 1317, 'valentines': 1318, 'village': 1319, 'vry': 1320, 'wap': 1321, 'weather': 1322, 'wnt': 1323, 'woke': 1324, 'worries': 1325, 'xchat': 1326, 'yay': 1327, 'yijue': 1328, '     ': 1329, 'action': 1330, 'ad': 1331, 'adult': 1332, 'advice': 1333, 'affection': 1334, 'afraid': 1335, 'ahmad': 1336, 'air': 1337, 'aiyo': 1338, 'al': 1339, 'alert': 1340, 'alex': 1341, 'alrite': 1342, 'amazing': 1343, 'announcement': 1344, 'answers': 1345, 'apparently': 1346, 'arcade': 1347, 'arrested': 1348, 'asleep': 1349, 'assume': 1350, 'babes': 1351, 'bag': 1352, 'bahamas': 1353, 'basically': 1354, 'bcums': 1355, 'become': 1356, 'bedroom': 1357, 'begin': 1358, 'belly': 1359, 'bet': 1360, 'bitch': 1361, 'bloodsend': 1362, 'bloody': 1363, 'bowl': 1364, 'boys': 1365, 'bringing': 1366, 'broke': 1367, 'bugis': 1368, 'buns': 1369, 'bye': 1370, 'cal': 1371, 'callers': 1372, 'calls£': 1373, 'cancel': 1374, 'carefully': 1375, 'cartoon': 1376, 'cc': 1377, 'center': 1378, 'changed': 1379, 'children': 1380, 'clean': 1381, 'closer': 1382, 'colleagues': 1383, 'conditions': 1384, 'connect': 1385, 'convey': 1386, 'cry': 1387, 'darling': 1388, 'december': 1389, 'deleted': 1390, 'dictionary': 1391, 'died': 1392, 'diet': 1393, 'dirty': 1394, 'discuss': 1395, 'doctor': 1396, 'don\\x92t': 1397, 'dropped': 1398, 'due': 1399, 'earth': 1400, 'easier': 1401, 'eatin': 1402, 'embarassed': 1403, 'ending': 1404, 'eng': 1405, 'er': 1406, 'exact': 1407, 'exams': 1408, 'expect': 1409, 'expecting': 1410, 'expensive': 1411, 'fa': 1412, 'fall': 1413, 'fantasy': 1414, 'fb': 1415, 'fighting': 1416, 'figure': 1417, 'followed': 1418, 'football': 1419, 'freak': 1420, 'freefone': 1421, 'frndship': 1422, 'fromm': 1423, 'fullonsmscom': 1424, 'gals': 1425, 'gee': 1426, 'geeee': 1427, 'girlfrnd': 1428, 'gives': 1429, 'godi': 1430, 'google': 1431, 'grand': 1432, 'gravity': 1433, 'grl': 1434, 'hanging': 1435, 'happens': 1436, 'havnt': 1437, 'hip': 1438, 'hl': 1439, 'holder': 1440, 'holla': 1441, 'horrible': 1442, 'iam': 1443, 'idk': 1444, 'imagine': 1445, 'indians': 1446, 'informed': 1447, 'itplspls': 1448, 'izzit': 1449, 'i‘m': 1450, 'jada': 1451, 'jiu': 1452, 'jokes': 1453, 'keeping': 1454, 'kettoda': 1455, 'kick': 1456, 'kusruthi': 1457, 'ladies': 1458, 'law': 1459, 'learn': 1460, 'library': 1461, 'lie': 1462, 'lik': 1463, 'listening': 1464, 'local': 1465, 'locations': 1466, 'login': 1467, 'london': 1468, 'looked': 1469, 'looks': 1470, 'loverboy': 1471, 'malaria': 1472, 'manda': 1473, 'marriage': 1474, 'marry': 1475, 'matured': 1476, 'meal': 1477, 'meetin': 1478, 'melle': 1479, 'met': 1480, 'mid': 1481, 'mmm': 1482, 'moby': 1483, 'model': 1484, 'mood': 1485, 'mths': 1486, 'naked': 1487, 'naughty': 1488, 'net': 1489, 'none': 1490, 'ordered': 1491, 'others': 1492, 'oz': 1493, 'pages': 1494, 'partner': 1495, 'passionate': 1496, 'pin': 1497, 'pissed': 1498, 'plenty': 1499, 'polyphonic': 1500, 'ppl': 1501, 'prepare': 1502, 'present': 1503, 'purpose': 1504, 'putting': 1505, 'q': 1506, 'qatar': 1507, 'questions': 1508, 'quote': 1509, 'raining': 1510, 'rally': 1511, 'ran': 1512, 'random': 1513, 'reasons': 1514, 'receiving': 1515, 'records': 1516, 'red': 1517, 'refused': 1518, 'relation': 1519, 'remind': 1520, 'replied': 1521, 'report': 1522, 'result': 1523, 'results': 1524, 'return': 1525, 'revealed': 1526, 'ringtones': 1527, 'roger': 1528, 'rooms': 1529, 'ru': 1530, 'salary': 1531, 'sale': 1532, 'santa': 1533, 'scream': 1534, 'screaming': 1535, 'serious': 1536, 'settled': 1537, 'shame': 1538, 'shd': 1539, 'shell': 1540, 'sick': 1541, 'sight': 1542, 'sn': 1543, 'somethin': 1544, 'songs': 1545, 'sonyericsson': 1546, 'sore': 1547, 'spl': 1548, 'spoken': 1549, 'stopped': 1550, 'str': 1551, 'stylish': 1552, 'subscriber': 1553, 'subscription': 1554, 'suite': 1555, 'sup': 1556, 'super': 1557, 'takin': 1558, 'tampa': 1559, 'tariffs': 1560, 'teach': 1561, 'tells': 1562, 'tenants': 1563, 'texted': 1564, 'tihs': 1565, 'tkts': 1566, 'tncs': 1567, 'torch': 1568, 'total': 1569, 'tough': 1570, 'tour': 1571, 'transaction': 1572, 'truly': 1573, 'ttyl': 1574, 'tuition': 1575, 'turn': 1576, 'tyler': 1577, 'urself': 1578, 'vote': 1579, 'walking': 1580, 'weed': 1581, 'weird': 1582, 'wet': 1583, 'wherever': 1584, 'whether': 1585, 'white': 1586, 'willing': 1587, 'woman': 1588, 'works': 1589, 'worse': 1590, 'yan': 1591, 'yer': 1592, 'youclean': 1593, 'yuo': 1594, 'zed': 1595, '–': 1596, 'aah': 1597, 'aathilove': 1598, 'abi': 1599, 'acc': 1600, 'accident': 1601, 'accounts': 1602, 'ache': 1603, 'activate': 1604, 'activities': 1605, 'addie': 1606, 'aftr': 1607, 'ages': 1608, 'ahead': 1609, 'aiya': 1610, 'aiyar': 1611, 'along': 1612, 'amount': 1613, 'ansr': 1614, 'answering': 1615, 'anybody': 1616, 'anyways': 1617, 'app': 1618, 'appointment': 1619, 'appt': 1620, 'armand': 1621, 'arms': 1622, 'arng': 1623, 'arrange': 1624, 'art': 1625, 'asks': 1626, 'atlanta': 1627, 'attend': 1628, 'audition': 1629, 'auto': 1630, 'avent': 1631, 'bat': 1632, 'bath': 1633, 'bb': 1634, 'becoz': 1635, 'beer': 1636, 'befor': 1637, 'bell': 1638, 'belovd': 1639, 'bf': 1640, 'biggest': 1641, 'billed': 1642, 'birds': 1643, 'birla': 1644, 'black': 1645, 'blah': 1646, 'blame': 1647, 'blessings': 1648, 'bloo': 1649, 'bmw': 1650, 'boat': 1651, 'bold': 1652, 'boo': 1653, 'boring': 1654, 'boston': 1655, 'bottle': 1656, 'bought': 1657, 'boye': 1658, 'brain': 1659, 'brand': 1660, 'brilliant': 1661, 'bro': 1662, 'brothas': 1663, 'brought': 1664, 'btnationalrate': 1665, 'bud': 1666, 'building': 1667, 'burger': 1668, 'buzy': 1669, 'cabin': 1670, 'calicut': 1671, 'callback': 1672, 'callin': 1673, 'cam': 1674, 'campus': 1675, 'career': 1676, 'carry': 1677, 'cars': 1678, 'catching': 1679, 'cbe': 1680, 'celebrate': 1681, 'cell': 1682, 'cha': 1683, 'chain': 1684, 'challenge': 1685, 'chasing': 1686, 'chatting': 1687, 'cheer': 1688, 'china': 1689, 'chinese': 1690, 'choice': 1691, 'chosen': 1692, 'city': 1693, 'click': 1694, 'closedincluding': 1695, 'coins': 1696, 'comedy': 1697, 'common': 1698, 'competition': 1699, 'connection': 1700, 'contents': 1701, 'cooking': 1702, 'cornwall': 1703, 'costa': 1704, 'costs': 1705, 'cost£': 1706, 'crack': 1707, 'cramps': 1708, 'cream': 1709, 'created': 1710, 'credited': 1711, 'cross': 1712, 'croydon': 1713, 'cruise': 1714, 'cud': 1715, 'cust': 1716, 'dave': 1717, 'decision': 1718, 'def': 1719, 'delivered': 1720, 'dem': 1721, 'di': 1722, 'dick': 1723, 'doggy': 1724, 'don‘t': 1725, 'doors': 1726, 'dress': 1727, 'drinking': 1728, 'drinks': 1729, 'drug': 1730, 'dry': 1731, 'dsnt': 1732, 'during': 1733, 'eerie': 1734, 'em': 1735, 'ended': 1736, 'enemy': 1737, 'enjoyed': 1738, 'escape': 1739, 'ese': 1740, 'esplanade': 1741, 'essential': 1742, 'euro': 1743, 'exe': 1744, 'executive': 1745, 'exhausted': 1746, 'explain': 1747, 'express': 1748, 'facebook': 1749, 'failed': 1750, 'fancies': 1751, 'faster': 1752, 'fathima': 1753, 'fault': 1754, 'fear': 1755, 'february': 1756, 'feet': 1757, 'fetch': 1758, 'fever': 1759, 'fifteen': 1760, 'files': 1761, 'fills': 1762, 'finishes': 1763, 'finishing': 1764, 'flaked': 1765, 'flight': 1766, 'flights': 1767, 'fml': 1768, 'force': 1769, 'foreign': 1770, 'fran': 1771, 'freezing': 1772, 'frying': 1773, 'funny': 1774, 'further': 1775, 'gap': 1776, 'gautham': 1777, 'generally': 1778, 'germany': 1779, 'giv': 1780, 'given': 1781, 'gmgngegn': 1782, 'gona': 1783, 'goodnite': 1784, 'gossip': 1785, 'govtinstituitions': 1786, 'gram': 1787, 'green': 1788, 'greet': 1789, 'h': 1790, 'hahahause': 1791, 'haiz': 1792, 'hands': 1793, 'happend': 1794, 'hate': 1795, 'havin': 1796, 'hella': 1797, 'hint': 1798, 'holding': 1799, 'homeowners': 1800, 'hon': 1801, 'honey': 1802, 'honeybee': 1803, 'hostel': 1804, 'housemaid': 1805, 'ibiza': 1806, 'ideas': 1807, 'ikea': 1808, 'inches': 1809, 'incident': 1810, 'including': 1811, 'inclusive': 1812, 'indian': 1813, 'inform': 1814, 'innings': 1815, 'instantly': 1816, 'intelligent': 1817, 'intro': 1818, 'iscoming': 1819, 'issue': 1820, 'italian': 1821, 'i\\x92m': 1822, 'i‘ll': 1823, 'james': 1824, 'january': 1825, 'jason': 1826, 'jazz': 1827, 'jess': 1828, 'jesus': 1829, 'joke': 1830, 'july': 1831, 'june': 1832, 'kadeem': 1833, 'karaoke': 1834, 'kerala': 1835, 'key': 1836, 'ki': 1837, 'kisses': 1838, 'kkim': 1839, 'knackered': 1840, 'knowing': 1841, 'la': 1842, 'lacsthats': 1843, 'lacsthere': 1844, 'laid': 1845, 'largest': 1846, 'laughed': 1847, 'lecture': 1848, 'legal': 1849, 'letter': 1850, 'letters': 1851, 'lick': 1852, 'lido': 1853, 'lifetime': 1854, 'lives': 1855, 'lmao': 1856, 'logo': 1857, 'lonely': 1858, 'losing': 1859, 'loss': 1860, 'lotr': 1861, 'mad': 1862, 'maga': 1863, 'meaning': 1864, 'meanwhile': 1865, 'meds': 1866, 'mei': 1867, 'membership': 1868, 'messageno': 1869, 'messenger': 1870, 'midnight': 1871, 'mite': 1872, 'moji': 1873, 'moments': 1874, 'monthly': 1875, 'mother': 1876, 'moved': 1877, 'mt': 1878, 'mummy': 1879, 'names': 1880, 'nan': 1881, 'nearly': 1882, 'nights': 1883, 'norm': 1884, 'ntwk': 1885, 'nyc': 1886, 'officeunderstand': 1887, 'oic': 1888, 'oni': 1889, 'onwards': 1890, 'original': 1891, 'outta': 1892, 'pack': 1893, 'package': 1894, 'page': 1895, 'pence': 1896, 'perfect': 1897, 'personality': 1898, 'pg': 1899, 'pie': 1900, 'pig': 1901, 'pilates': 1902, 'places': 1903, 'players': 1904, 'pleased': 1905, 'pod': 1906, 'pongal': 1907, 'porn': 1908, 'possession': 1909, 'possible': 1910, 'postcode': 1911, 'posted': 1912, 'potential': 1913, 'prabhaim': 1914, 'previously': 1915, 'promises': 1916, 'provided': 1917, 'ps': 1918, 'public': 1919, 'purchase': 1920, 'pussy': 1921, 'quoting': 1922, 'railway': 1923, 'raise': 1924, 'randomly': 1925, 'rays': 1926, 'realize': 1927, 'received': 1928, 'register': 1929, 'regular': 1930, 'released': 1931, 'remembered': 1932, 'responcewhat': 1933, 'respond': 1934, 'responding': 1935, 'returns': 1936, 'rich': 1937, 'ride': 1938, 'ron': 1939, 'roommate': 1940, 'row': 1941, 'rule': 1942, 'runs': 1943, 'rush': 1944, 'sam': 1945, 'saucy': 1946, 'saved': 1947, 'scared': 1948, 'score': 1949, 'scotland': 1950, 'screen': 1951, 'se': 1952, 'season': 1953, 'sec': 1954, 'sed': 1955, 'seem': 1956, 'selection': 1957, 'selling': 1958, 'sen': 1959, 'sept': 1960, 'settle': 1961, 'seven': 1962, 'sha': 1963, 'shortage': 1964, 'shortly': 1965, 'shut': 1966, 'sighs': 1967, 'sit': 1968, 'site': 1969, 'skilgme': 1970, 'skip': 1971, 'skype': 1972, 'smiles': 1973, 'smokes': 1974, 'soft': 1975, 'software': 1976, 'sol': 1977, 'solve': 1978, 'sometime': 1979, 'sooner': 1980, 'sory': 1981, 'sorydarealyfrm': 1982, 'source': 1983, 'sp': 1984, 'space': 1985, 'spanish': 1986, 'spoke': 1987, 'spook': 1988, 'sports': 1989, 'sptyrone': 1990, 'stamps': 1991, 'standard': 1992, 'standing': 1993, 'stars': 1994, 'step': 1995, 'stock': 1996, 'stomach': 1997, 'strong': 1998, 'subs': 1999, 'successfully': 2000, 'sunny': 2001, 'sura': 2002, 'sweetest': 2003, 'taunton': 2004, 'tb': 2005, 'teacher': 2006, 'team': 2007, 'teasing': 2008, 'term': 2009, 'thangam': 2010, 'thnk': 2011, 'thts': 2012, 'thurs': 2013, 'thursday': 2014, 'tick': 2015, 'ticket': 2016, 'timing': 2017, 'tis': 2018, 'tm': 2019, 'tom': 2020, 'touched': 2021, 'travel': 2022, 'tt': 2023, 'turns': 2024, 'twelve': 2025, 'twice': 2026, 'txtin': 2027, 'umma': 2028, 'unable': 2029, 'uncles': 2030, 'understood': 2031, 'unsub': 2032, 'upload': 2033, 'urgentbut': 2034, 'urgentlyits': 2035, 'urgnt': 2036, 'usually': 2037, 'valuable': 2038, 'various': 2039, 'version': 2040, 'vijay': 2041, 'vikky': 2042, 'voda': 2043, 'vodka': 2044, 'vomit': 2045, 'wa': 2046, 'wah': 2047, 'waitu': 2048, 'walls': 2049, 'walmart': 2050, 'warner': 2051, 'weak': 2052, 'welp': 2053, 'we‘re': 2054, 'whenevr': 2055, 'wifehow': 2056, 'wins': 2057, 'wkend': 2058, 'wn': 2059, 'women': 2060, 'wonders': 2061, 'wtf': 2062, 'wun': 2063, 'xavier': 2064, 'xxxx': 2065, 'ym': 2066, 'yor': 2067, 'é': 2068, 'üll': 2069})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlHEofOQJ9Ze",
        "colab_type": "text"
      },
      "source": [
        "**Preparing  batches for training the mode using BucketIterator**\n",
        " It defines an iterator that batches examples of similar lengths together.\n",
        "Minimizes amount of padding needed while producing freshly shuffled batches for each new epoch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxBiOFMfpD60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "#Load an iterator\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.text),\n",
        "    sort_within_batch=True,\n",
        "    device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44UenskHrhnU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1705b578-a290-46c4-a1e0-67927e76410d"
      },
      "source": [
        "print(vars(valid_data.examples[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'label': '0', 'text': ['alex', 'says', 'he', 's', 'not', 'ok', 'with', 'you', 'not', 'being', 'ok', 'with', 'it']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtMIt7t3YKvE",
        "colab_type": "text"
      },
      "source": [
        "# Model Architecture\n",
        "![Untitled Diagram.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1sAaPTriMBr",
        "colab_type": "text"
      },
      "source": [
        "The above shown picture is our model architecture, here we are defining a class ClassifierModel which is a subclass of nn.Module class which is Base class for all neural network modules.\n",
        " It will have two functions init function, and forward function\n",
        "* INIT function : it contains the constructor of the class it will initialises the componnent of the class. Whenever an instance of a class is created, init function is automatically invoked. Hence, it is called as a constructor.\n",
        "* Forward function:- Forward function defines the forward pass of the inputs.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**COMPONENTS OF MODEL **\n",
        "\n",
        "1. **Embedding layer**:- In embedding layer embeddings individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network. *Key to the approach is the idea of using a dense distributed representation for each word.*. It takes 2 input  \n",
        "    1.   Input dimension:- number of unique words in the vocabulary\n",
        "    2.   Embedding dimension:- length in which vector of each word embedding is represented\n",
        "\n",
        "2. **RNN layer**:- It is a Rnn which reads the word sequence . It takes the following input parameters:-\n",
        "    1. embedding_dim - length of each embedding vector or input sequence\n",
        "    2. Hidden_dim- it defines the number of hidden nodes in RNN\n",
        "    3. num_layers - Number of RNN layers to be stacked\n",
        "\n",
        "3. **Linear layer**- or dense layer, it takes two input \n",
        "\n",
        "    1. hidden_ dim= number of input feature\n",
        "    2. output_dim=  number of output feature\n",
        "\n",
        "4.   **Pack padding sequences**:- pytorch allows us to pack the sequence, internally packed sequence is a tuple of two lists. One contains the elements of sequences. Elements are interleaved by time steps (see example below) and other containsthe batch size at each step. This is helpful in recovering the actual sequences as well as telling RNN what is the batch size at each time step.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWu_esTunXI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class ClassifierModel(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim,num_layers=2)\n",
        "        self.fc1 = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.act= nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text,text_lengths):\n",
        "        # text [sentence length, batch_size]\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        #packed sequence\n",
        "        text_lengths_clamped = text_lengths.clamp(min=1, max=100)\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths_clamped,batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        \n",
        "                \n",
        "        output = self.fc1(hidden)\n",
        "        output = self.dropout(output)\n",
        "        output= self.act(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_q9Jw2csQdx",
        "colab_type": "text"
      },
      "source": [
        "Initialising parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9vWGPbaofNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = len(TEXT.vocab)\n",
        "embedding_dim = 100\n",
        "num_hidden_nodes = 32\n",
        "num_output_nodes = 1\n",
        "\n",
        "#instantiate the model\n",
        "model = ClassifierModel(input_dim, embedding_dim, num_hidden_nodes,num_output_nodes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7K6XnSKsYlI",
        "colab_type": "text"
      },
      "source": [
        "Visualising the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78Ku7e0SozWz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "5865835f-75f2-482c-e11e-0655def0c3b4"
      },
      "source": [
        "\n",
        "#architecture\n",
        "print(model)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ClassifierModel(\n",
            "  (embedding): Embedding(2070, 100)\n",
            "  (rnn): RNN(100, 32, num_layers=2)\n",
            "  (fc1): Linear(in_features=32, out_features=1, bias=True)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (act): Sigmoid()\n",
            ")\n",
            "The model has 213,433 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PnWhunSseaM",
        "colab_type": "text"
      },
      "source": [
        "Initialising the embedding layer with pretrained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0H3jHMxzATR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "31abb6a7-52c0-487e-c27a-27592d881a2d"
      },
      "source": [
        "#Initialize the pretrained embedding\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        ...,\n",
            "        [-0.6663,  0.0492,  0.2599,  ..., -0.6937,  0.0191, -0.0656],\n",
            "        [-0.1554,  0.3377, -0.2220,  ..., -0.8515, -0.3765,  0.0668],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAjNKEbpszxX",
        "colab_type": "text"
      },
      "source": [
        "Defining optimizer and loss function fo training:- \n",
        "\n",
        "Optimizer is adam optimizer\n",
        "and loss is BCEwithLogit loss for binary classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc_gryq5pVMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "EPOCHS= 10\n",
        "#define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fr2LcqXtJ72",
        "colab_type": "text"
      },
      "source": [
        "# Training and Evaluating Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woIKpa432Xps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch(model,iterator,criterion,optimizer,device, n_examples):\n",
        "    model= model.train()\n",
        "    losses = []\n",
        "    for  batch in iterator:\n",
        "        #retrieve text and no. of words\n",
        "        text, text_lengths = batch.text \n",
        "        outputs = model(text,text_lengths).squeeze(1)\n",
        "        loss = criterion(outputs, batch.label)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    return np.mean(losses)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSq3hIu7phVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_model(model, iterator, criterion, device, n_examples):\n",
        "    \n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for  batch in iterator:\n",
        "           text, text_lengths = batch.text\n",
        "           outputs = model(text,text_lengths).squeeze(1)\n",
        "           loss = criterion(outputs, batch.label)\n",
        "           losses.append(loss.item())\n",
        "      \n",
        "        return np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LLmYN3d3sX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye2XFExktcNu",
        "colab_type": "text"
      },
      "source": [
        "Printing the loss for each epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deUi6-ap3xMC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 926
        },
        "outputId": "11d5d643-edd5-4bfc-d660-7fbbef5d35bf"
      },
      "source": [
        "\n",
        "history = defaultdict(list)\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "    print('-' * 10)\n",
        "    train_loss = train_epoch(model,train_iterator,criterion,optimizer,device,len(train_iterator))\n",
        "    print(f'Train loss {train_loss}')\n",
        "    val_loss = eval_model(model,valid_iterator,criterion,device,len(valid_iterator))\n",
        "    print(f'Val   loss {val_loss} ')\n",
        "    print()\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "----------\n",
            "Train loss 0.7738257714959441\n",
            "Val   loss 0.7100893612261172 \n",
            "\n",
            "Epoch 2/10\n",
            "----------\n",
            "Train loss 0.7375994441939182\n",
            "Val   loss 0.702339752956673 \n",
            "\n",
            "Epoch 3/10\n",
            "----------\n",
            "Train loss 0.736222930618974\n",
            "Val   loss 0.6978233191702101 \n",
            "\n",
            "Epoch 4/10\n",
            "----------\n",
            "Train loss 0.7296022313540099\n",
            "Val   loss 0.6861234616350245 \n",
            "\n",
            "Epoch 5/10\n",
            "----------\n",
            "Train loss 0.7152435242152605\n",
            "Val   loss 0.6677590409914652 \n",
            "\n",
            "Epoch 6/10\n",
            "----------\n",
            "Train loss 0.7067380742948564\n",
            "Val   loss 0.6710451730975399 \n",
            "\n",
            "Epoch 7/10\n",
            "----------\n",
            "Train loss 0.7031934456747087\n",
            "Val   loss 0.664336849142004 \n",
            "\n",
            "Epoch 8/10\n",
            "----------\n",
            "Train loss 0.702530002007719\n",
            "Val   loss 0.6618876501365945 \n",
            "\n",
            "Epoch 9/10\n",
            "----------\n",
            "Train loss 0.6998005636402818\n",
            "Val   loss 0.6634305185741849 \n",
            "\n",
            "Epoch 10/10\n",
            "----------\n",
            "Train loss 0.7040034161239374\n",
            "Val   loss 0.6628757030875595 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ualFvA1TtjKm",
        "colab_type": "text"
      },
      "source": [
        "# Prediction on a random \n",
        "Firstly we have to put the model at Evaluation mode.\n",
        "\n",
        "We'll define a function nmaed predict which will take up an input sentence and will predict wether the sentence is a spam mail or ham mail,\n",
        "\n",
        "The compnent of the functions are :\n",
        "1. It takes up the text and clean it (remove special characters etc)\n",
        "2. then it tokenize the sentence\n",
        "3. Then it converts the tokenized sequence into a tensor\n",
        "4. and then it calculates the length of tense\n",
        "5. It passes both sequence of integers and length to model \n",
        "6. we take the maximum of the result and produce the prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZkF11zl1dSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def predict(model, sentence):\n",
        "    sentence= clean_text(sentence)\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]  #tokenize the sentence \n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]          #convert to integer sequence\n",
        "    length = [len(indexed)]                                    #compute no. of words\n",
        "    tensor = torch.LongTensor(indexed).to(device)              #convert to tensor\n",
        "    tensor = tensor.unsqueeze(1).T                             #reshape in form of batch,no. of words\n",
        "    length_tensor = torch.LongTensor(length)                   #convert to tensor\n",
        "    prediction = model(tensor, length_tensor)                  #prediction \n",
        "    preds= torch.max(prediction)\n",
        "    if (preds>0.005):\n",
        "      result= 'spam!!'\n",
        "    else:\n",
        "      result= 'ham!'\n",
        "\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwA2klKi90iy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b3ef3493-45b9-4f40-c0ce-fbc4116d591f"
      },
      "source": [
        "predict(model, \"Are you okay\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ham!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW5b8MFX94wY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "134cb313-e025-472b-fc2b-d11a1bfb06a3"
      },
      "source": [
        "predict(model,\"Free massage coupons\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'spam!!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    }
  ]
}